\section{Introduction}
\label{sec:intro}
\subsection{Forward Error Correction schemes}
Forward Error Correction (FEC) coding schemes are used extensively in almost every communication and data processing system, in order to increase the reliability of transmission and storage of data. This is especially important in space communications scenarios, due to the extremely stringent SNR requirements of deep-space links, or conversely, the high data rates and low latency necessary in near-earth satellite communication scenarios. At the same time, an efficient FEC scheme implementation in a realistic scenario has to balance contradicting requirements and offer a variety of trade-offs in terms of error correcting efficiency, encoding/decoding complexity, throughput, hardware resources utilization and power consumption.\par
The traditional approach which has been widely adopted since the dawn of digital communications implements countermeasures against errors caused by noise, distortion and interference,  at the symbol level. We refer to this component of the communication system as channel coding \cite{Ryan2009}. Almost every modern communication standard and system includes a channel coding scheme, at least as an option. Well known schemes in this area include Reed-Solomon (RS) \cite{Reed1960} and Turbo \cite{Berrou1993} codes. Another highly advantageous class of channel codes are the Low-Density Parity-Check (LDPC) codes, which are linear block codes, characterized by large block lengths and sparse parity-check matrices. Introduced by R.G. Gallager in 1960 \cite{Gallager1962}, LDPC codes had in following years generally succumbed to oblivion, due to the current eraâ€™s technology limitations, which could not allow their implementation at a reasonable cost. However, advances in VLSI technology, together with the application of efficient code design techniques have annihilated those barriers. Among the entire range of modern error correcting codes (ECC), they are currently the most promising approach towards the capacity limit described by Shannon \cite{Shannon}. This has established them as the optimal choice for FEC in modern applications.\par

The initial Gallager codes were random and although they exhibited excellent error-correcting capabilities, hardware implementation was challenging. In order to reduce implementation complexity and encoding/decoding speed, additional structure has been designed into the parity check matrices of all practical LDPC codes in modern applications, so that they consist of an array of juxtaposed cyclic sub matrices, named the circulants, which can be efficiently implemented. These structured codes are collectively referred to as Quasi-Cyclic (QC) LDPC codes. QC-LDPC codes have been adopted by many modern communication standards, such as IEEE 802.11, 802.16 and DVB-S2. A special class of structured LDPC codes, the protograph-based QC codes have recently received considerable research interest in many modern standards. QC LDPC codes have also been adopted by the Consultative Committee for Space Data Systems (CCSDS) as recommended standard for on-board channel coding in Near-Earth and Deep-Space communications \cite{CCSDS131.0}.\par
A multitude of encoder architectures for QC-LDPC codes has been proposed in the literature, while several products are available in the market. Most of the proposed architectures though are optimized for the specific codes adopted by corresponding standards, leveraging the specific properties if the particular code structure. Especially those demonstrating practical throughput in the range of multiple Gbps, they are either entirely not applicable or are not expected to scale with CCSDS codes, the parity-check matrix of which do not exhibit the required structure.\par

The error correcting capability of bit-level channel coding however is limited in high speed and deep fading scenarios, such as those encountered in modern earth-to-satellite and satellite-to-satellite laser links. Moreover, these environments are characterized by high latency and traditional automatic repeat request (ARQ) schemes are not applicable or practical, or the fading effect of the communication channel is so deep that bit-level channel codes cannot provide the required reliability, since even a single scintillation effect can span multiple entire transmitted codewords. In these cases, the erroneously received or completely missed codewords can be considered as erased symbols and the most suitable model for the communication channel is the block erasure channel. Error correction in this case takes place at a higher level of the communications protocol stack than bit-level channel coding (which is typically a function of the data link layer in OSI protocol stack). Erased symbols are entire packets of the underlying protocol.\par

A common approach for coding over block erasure channels is the combination of RS codes with interleaving. RS codes are maximum distance separable (MDS) codes: if $(n,k)$ are the dimensions of a code, it can recover from the erasure of any $n-k$ or fewer symbols. Consequently, they can provide optimal error recovery capability. An interleaver is typically connected to the output of the RS encoder in order to protect against deep fading. Such coding schemes have been proposed in \cite{CCSDS141} and \cite{CCSDS142} for optical space communications. The limitation of RS codes, however, is high encoding complexity, which imposes the use of short block lengths. The polynomial arithmetic operations involved in encoding and decoding operations result in non-linear encoding/decoding complexity, even in the base case proposed in \cite{Tang2020}. The use of RaptorQ codes has alternatively been proposed in \cite{Adhikary2020}.\par
Another promising approach is the use of packet-level LDPC erasure codes, according to which encoded symbols are entire blocks of information bits. Although these codes are not MDS, capacity approaching ensembles can perform very close to the Singleton bound \cite{GuilleniFabregas2006} and encoder and decoder complexity can scale linearly with block length. Packet-level LDPC erasure coding has also been proposed in \cite{CCSDS131.5} for near-earth and deep space communications.\par




\subsection{On-board data processing}\label{subsec:Onboard}
The stringent requirements of aerospace applications in terms of reliability and power call for a different approach, when considering on-board data processing equipment. Commercial devices, targeting hugely larger market shares and lower time to market,  cannot obviously meet these requirements. Processors in space are required to withstand harsh environmental conditions, mainly due to radiation effects. In addition, the risk margin of the disruption of the mission needs to be significantly lower. To meet these ends, the space industry has established the notion of Technology Readiness Level, and the relevant guidelines for ECSS are provided in \cite{ECSS-E-HB-11A}.\par
The degradation of the reliability of electronic systems manifests itself in two forms of errors in their operation. The most severe form refers to hard errors. These can happen as a consequence of the gradual or sudden degradation of the system caused by the accumulation or a surge of total ionizing dose (TID) or atomic displacement (Total Non Ionizing Dose-TNID or Displacement Damage-DD) \cite{Ecoffet2013}. Another kind of effects are transient phenomena which lead to so-called "soft errors" in the component's operation. When the error in the system is caused by the passage of a single particle, the event can be categorised as Single Event Effect (SEE). SEEs can lead to soft errors, for example Single Event Upsets \cite{George2019}, or hard, as is the case with Single Event Upsets (SEU) or Burnouts (SEB).\par
Depending on the type of the effect, various mitigation techniques are applied at various levels: from the physical layer, which refers to the semiconductor fabrication process up to the system level design. Devices employing these techniques are referred to as radiation tolerant, or radiation hardened devices. Radiation hardening aims to minimise the probability of radiation effect's occurrence in the first place, mostly by measures on the physical layer and their cost of radiation hardened can be significantly higher than that of their commercial counterparts. Radiation tolerance, on the other hand, assumes that radiation effects are bound to occur and aims at reducing the impact of radiation effects on the system's operation. ECC in the memories and buses is the fundamental radiation tolerance technique. A summary of mitigation techniques at various levels of design can be found in \cite{HUANG2019105} and the references therein.\par
The topic of mitigation techniques is widely covered in the literature. Consequently, we limit our brief description to the following techniques, which are more relevant to this work: Triple Modular Redundancy (TMR) and memory scrubbing. In a basic TMR sheme, three redundant circuits perform the same task on the same data. A majority vote process at the system's output can mask a failure in one of the circuits. Obviously, the cost of this approach is that it requires triple resources. Memory scrubbing, as the name implies, is a method to increase the integrity of data stored in a memory system. It requires that a method of ECC has been applied to the data written in the memory. Its contents are periodically retrieved, any errors are detected and corrected with the ECC and the result is written back to the memory. The frequency of the memory scans needs to be balanced, so that single errors are not accumulated and the ECC fails.\par
Typical on-board data handling systems are built around a central processor (OBC-On Board Data Computer), which is mostly responsible for telecommand functions and the coordination of the rest of the platform subsystems: telecommunication, telemetry, mass memory subsystems, sensors, instruments,  and payload processors. All these subsystems communicate through highly reliable communication links, typically MIL-STD-1553, or spacewire and spacefibre, which are described separately in Section \ref{subsubsec:spfi}. The Space Avionics Open Interface Architecture (SAVOIR) initiative is a move towards the standardization of space avionics and, among other products, it proposes a reference functional architecture reference model.\par
In the near future space computing technology is expected to converge more rapidly with terrestrial practices \cite{Furano2018}, so that, depending on the mission goals, the required balance between performance, resiliency and cost is met: as smaller payloads with a limited lifespan are becoming more popular, the requirements for space-qualified parts can be relaxed. The most extreme example of this scenario is the case with cubesats and nanosattelites in the "new space" emerging trend \cite{newspace}, the lifespan of which can be as small as a few days \cite{Oltrogge}. In this aspect, for non-mission critical functions and time-specific payloads, even the use of COTS equipment can be acceptable.\par


\subsection{Space communication channels, systems \& protocols}
near-earth and deep space channel modelling. Then about erasure channel 
Fundamentals: Capacity, FEC basics, channel models

